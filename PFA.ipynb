{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sts\n",
    "from scipy.special import gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_bino_r(k,r,p):\n",
    "    # To calculate the probability of k .\n",
    "    # k is the times of success , r is the times of failure , p is the probability of success\n",
    "    y = gamma(r+k)/(gamma(k+1)*gamma(r)) * (1-p)**r  * p**k\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('lda.npz')\n",
    "data_train = data['train']\n",
    "data_test = data['test']\n",
    "\n",
    "data_train = data_train[data_train[:,0]<200]\n",
    "data_test = data_train[data_train[:,0]<50]\n",
    "\n",
    "data_train[:,0] = data_train[:,0]-1\n",
    "data_test[:,0] = data_test[:,0]-1\n",
    "data_train[:,1] = data_train[:,1]-1\n",
    "data_test[:,1] = data_test[:,1]-1\n",
    "\n",
    "voc = max(data_train[:,1])+1\n",
    "doc = max(data_train[:,0])+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFA():\n",
    "    def __init__(self,voc,doc,data_train,data_test,eta=0.5,c=1,c0=1,r0=1,gamma=1,alpha=0.05,eps=0.05,k=10):\n",
    "        #initialization\n",
    "        #hyperparameter\n",
    "        self.eta = eta\n",
    "        self.c = c\n",
    "        self.c0 = c0\n",
    "        self.r0 = r0\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        \n",
    "        #number of document,word type,topic\n",
    "        self.k = k #主题数量\n",
    "        self.voc = voc #词汇数\n",
    "        self.doc = doc #文档数\n",
    "        \n",
    "        #gengerative process sample\n",
    "        self.phi = sts.dirichlet.rvs([self.alpha]*self.voc,size = self.k)  #Topic-word matrix  (voc * k matrix)\n",
    "        self.pk = sts.beta.rvs(self.c*self.eps,self.c*(1-self.eps),size =self.k) # pk 的分布 1*k matrix\n",
    "        self.rk = sts.gamma.rvs(self.c0*self.r0,scale=1/self.c0,size=self.k) #rk 1*k matrix\n",
    "        self.ki = np.array([sts.gamma.rvs(self.rk,scale = self.pk/(1-self.pk)) for i in range(self.doc)]) # doc * k matrix\n",
    "        \n",
    "        #sampling count\n",
    "        self.xik = np.zeros((self.doc,self.k)) #第i个主题第k个topic\n",
    "        self.xpk = np.zeros((self.voc,self.k)) #第k个主题第p个词的数量\n",
    "        self.xk = np.zeros((self.k))           #第k个topic的数量\n",
    "        self.xi = np.zeros((self.doc))         #第i个文档的数量\n",
    "        \n",
    "        self.per_plot = []\n",
    "        \n",
    "        self.data_test = data_test\n",
    "        self.data_train = data_train\n",
    "        \n",
    "    def sampler(self):\n",
    "        #1.Count assignment\n",
    "        for doc_word_count in self.data_train:\n",
    "            \n",
    "            doc_index,word_index,word_count = doc_word_count #[0,7,2]\n",
    "            \n",
    "            prob =   self.phi[:,word_index] * self.ki[doc_index] # 单词在每个主题的概率 * 该文档每个主题的概率\n",
    "            prob/=sum(prob) \n",
    "            res = sts.multinomial.rvs(word_count,p=prob)\n",
    "                \n",
    "            self.xik[doc_index] += res   #第i个文档第k个topic的数量\n",
    "            self.xpk[word_index] += res          #第k个主题第p个词的数量\n",
    "            self.xk += res                       #第k个topic的数量\n",
    "            self.xi[doc_index] += word_count     #第i个文档的数量\n",
    "        \n",
    "        #更新topic-doc  matrix,就是我们要替换的部分\n",
    "        for topic_index in range(self.k):\n",
    "            self.phi[topic_index] = sts.dirichlet.rvs([self.alpha]*self.voc + model.xpk[:,topic_index])\n",
    "            \n",
    "        #更新pk    \n",
    "        for topic_index in range(self.k):   \n",
    "            self.pk[topic_index] = sts.beta.rvs(self.c * self.eps + self.xk[topic_index] ,self.c *(1- self.eps)+ self.doc *  self.rk[topic_index] )\n",
    "        \n",
    "        #sample rk\n",
    "        for topic_index in range(self.k):    \n",
    "            if self.xk[topic_index] == 0: #当xk=0,负二项分布退化为伯努利分布\n",
    "                self.rk[topic_index] = sts.gamma.rvs(self.c0 * self.r0 , scale = 1/(self.c0 - self.doc * np.log(1- self.pk[topic_index])))\n",
    "            else:\n",
    "                self.rk[topic_index] = sts.gamma.rvs(self.c0 * self.r0 + np.sum(self.CRT(topic_index)),scale = 1/(self.c0 - self.doc * np.log(1- self.pk[topic_index])))\n",
    "        \n",
    "        # sample self.ki\n",
    "        for topic_index in range(self.k): \n",
    "            self.ki = sts.gamma(self.xik + self.rk,self.pk).rvs()\n",
    "     \n",
    "    def compute_perplexity(self):\n",
    "        phi_theta = np.array([np.sum(self.phi * self.ki[doc_index].reshape(-1,1),0) for doc_index in range(self.doc)])\n",
    "        phi_theta = phi_theta/np.sum(phi_theta,1).reshape(-1,1)            \n",
    "        mat = np.zeros((self.doc,self.voc)) \n",
    "        for index in self.data_test:#Convert sparse to normal matrix\n",
    "            mat[index[0],index[1]] = index[-1]    \n",
    "            per = np.sum(mat * np.log(phi_theta))\n",
    "            self.per_plot.append(np.exp(- per/np.sum(self.data_test[:,-1])))\n",
    "        return per\n",
    "    \n",
    "    def CRT(self,topic): #CRT flr \n",
    "        res = np.array([sum([sts.bernoulli.rvs(self.pk[topic]/(i-1+self.pk[topic])) for i in np.linspace(1,self.xik[d][topic],self.xik[d][topic])]) for d in range(self.doc)])\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def demo(self,iteration):\n",
    "        for it in range(iteration):\n",
    "            print (\"Initial complexity{/d}\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = PFA(voc=voc,doc=doc,data_train=data_train,data_test = data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: DeprecationWarning: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2094.574736957681]\n",
      "[2094.574736957681, 2059.525552303522]\n",
      "[2094.574736957681, 2059.525552303522, 2043.9965057438906]\n",
      "[2094.574736957681, 2059.525552303522, 2043.9965057438906, 2037.8941297625422]\n",
      "[2094.574736957681, 2059.525552303522, 2043.9965057438906, 2037.8941297625422, 2031.6604770995425]\n",
      "[2094.574736957681, 2059.525552303522, 2043.9965057438906, 2037.8941297625422, 2031.6604770995425, 2029.2605681724028]\n",
      "[2094.574736957681, 2059.525552303522, 2043.9965057438906, 2037.8941297625422, 2031.6604770995425, 2029.2605681724028, 2026.3726286490512]\n",
      "[2094.574736957681, 2059.525552303522, 2043.9965057438906, 2037.8941297625422, 2031.6604770995425, 2029.2605681724028, 2026.3726286490512, 2021.071645210049]\n",
      "[2094.574736957681, 2059.525552303522, 2043.9965057438906, 2037.8941297625422, 2031.6604770995425, 2029.2605681724028, 2026.3726286490512, 2021.071645210049, 2020.4832696317546]\n",
      "[2094.574736957681, 2059.525552303522, 2043.9965057438906, 2037.8941297625422, 2031.6604770995425, 2029.2605681724028, 2026.3726286490512, 2021.071645210049, 2020.4832696317546, 2019.4353625948174]\n",
      "[2094.574736957681, 2059.525552303522, 2043.9965057438906, 2037.8941297625422, 2031.6604770995425, 2029.2605681724028, 2026.3726286490512, 2021.071645210049, 2020.4832696317546, 2019.4353625948174, 2017.9449660640742]\n",
      "[2094.574736957681, 2059.525552303522, 2043.9965057438906, 2037.8941297625422, 2031.6604770995425, 2029.2605681724028, 2026.3726286490512, 2021.071645210049, 2020.4832696317546, 2019.4353625948174, 2017.9449660640742, 2017.293919154926]\n"
     ]
    }
   ],
   "source": [
    "res = model.perplexity(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
